{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run the optimization of the MERA (for small bond dimensions - only 2 -, even if we don't get the correct energy)\\\n",
    "contract the correct indices and then save them into an appropriate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bai/local/src/MERA-Hubbard/.venv/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import quimb as qu\n",
    "import quimb.tensor as qtn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cotengra as ctg\n",
    "\n",
    "from mera_hubbard import FH_Hamiltonian_NN_half_filling, solve_ground_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference ED at half-filling:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bai/local/src/MERA-Hubbard/.venv/lib/python3.12/site-packages/quimb/experimental/operatorbuilder/operatorbuilder.py:1431: NumbaTypeSafetyWarning: unsafe cast from int64 to undefined. Precision may be lost.\n",
      "  ci = bitmap[bi]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy =  -1.2360679774997894\n",
      "number of particles =  2.0\n",
      "\n",
      "\n",
      "New Hamiltonian at half-filling:\n",
      "chemical potential at half-filling =  -0.882\n",
      "SparseOperatorBuilder(nsites=4, nterms=10, locality=2))\n",
      "+ - . .  -1.0\n",
      "- + . .  -1.0\n",
      ". . + -  -1.0\n",
      ". . - +  -1.0\n",
      "n . n .  +2.0\n",
      "sn. . .  -0.8820000000001231\n",
      ". . sn.  -0.8820000000001231\n",
      ". n . n  +2.0\n",
      ". sn. .  -0.8820000000001231\n",
      ". . . sn -0.8820000000001231\n",
      "energy =  -1.2360679774997898\n",
      "number of particles =  2.0\n"
     ]
    }
   ],
   "source": [
    "num_sites = 2\n",
    "t1 = 1.\n",
    "t2 = 0.5 # trying without the NN terms first, as additional check\n",
    "U = 2.\n",
    "\n",
    "# FH HAMILTONIAN\n",
    "ham, sparse_ham = FH_Hamiltonian_NN_half_filling(num_sites, t1, t2, U, pbc=0) # pbc=0: open BC, pbc=1: cyclic BC\n",
    "ham.show()\n",
    "en, _, _ = solve_ground_state(num_sites, ham, sparse_ham)\n",
    "\n",
    "def norm_fn(mera):\n",
    "    # there are a few methods to do the projection\n",
    "    # exp works well for optimization\n",
    "    return mera.isometrize(method='exp')\n",
    "\n",
    "def expectation_sites(mera, local_terms, optimize='auto-qt'):\n",
    "    \"\"\"Compute the energy given the mera and the local terms of the hamiltonian\n",
    "    \"\"\"\n",
    "    # if we pass directly the hamiltonian the optimizer complains...\n",
    "    energy = 0.\n",
    "\n",
    "    for key in local_terms:\n",
    "        sites = []\n",
    "\n",
    "        term = local_terms[key]\n",
    "        \n",
    "        for item in key:\n",
    "            if item[0] == '↑':\n",
    "                sites.append(item[1]-1)\n",
    "            elif item[0] == '↓':\n",
    "                sites.append(item[1]-1 + int(ham.nsites/2))\n",
    "\n",
    "        # what gate should we apply?? (instead of 'term')\n",
    "        mera_op = mera.gate(term, sites, propagate_tags=False)\n",
    "        \n",
    "        mera_ex = mera_op & mera.H # apply the h.c. of the mera to calculate the expectation value\n",
    "\n",
    "        energy += mera_ex.contract(all, optimize=optimize) # calculate the expect. value\n",
    "    \n",
    "    return energy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bai/local/src/MERA-Hubbard/.venv/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:54: UserWarning: Couldn't find `optuna`, `baytune (btb)`, `chocolate`, `nevergrad` or `skopt` so will use completely random sampling in place of hyper-optimization.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*************** OPTIMIZATION MERA - MAX_BOND = 2 ***************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]/Users/bai/local/src/MERA-Hubbard/.venv/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "/Users/bai/local/src/MERA-Hubbard/.venv/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "/Users/bai/local/src/MERA-Hubbard/.venv/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "F=2.66 C=3.93 S=4.00 P=7.29:   6%|▋         | 1/16 [00:00<00:03,  3.77it/s]/Users/bai/local/src/MERA-Hubbard/.venv/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "/Users/bai/local/src/MERA-Hubbard/.venv/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "/Users/bai/local/src/MERA-Hubbard/.venv/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "/Users/bai/local/src/MERA-Hubbard/.venv/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "/Users/bai/local/src/MERA-Hubbard/.venv/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "F=2.66 C=3.93 S=4.00 P=7.29: 100%|██████████| 16/16 [00:00<00:00, 46.05it/s]\n",
      "F=2.66 C=3.93 S=4.00 P=7.29: 100%|██████████| 16/16 [00:00<00:00, 382.23it/s]\n",
      "F=2.66 C=3.93 S=4.00 P=7.29: 100%|██████████| 16/16 [00:00<00:00, 556.62it/s]\n",
      "F=2.62 C=3.93 S=4.00 P=7.17: 100%|██████████| 16/16 [00:00<00:00, 256.99it/s]\n",
      "F=2.62 C=3.93 S=4.00 P=7.17: 100%|██████████| 16/16 [00:00<00:00, 410.88it/s]\n",
      "F=2.66 C=3.93 S=4.00 P=7.29: 100%|██████████| 16/16 [00:00<00:00, 379.51it/s]\n",
      "F=2.62 C=3.93 S=4.00 P=7.17: 100%|██████████| 16/16 [00:00<00:00, 279.98it/s]\n",
      "F=2.62 C=3.93 S=4.00 P=7.17: 100%|██████████| 16/16 [00:00<00:00, 263.23it/s]\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m expectation_sites(mera, local_terms, optimize\u001b[38;5;241m=\u001b[39mopt)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# REAL OPTIMIZATION\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# set-up the MERA optimizer object:\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m tnopt \u001b[38;5;241m=\u001b[39m \u001b[43mqtn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTNOptimizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmera\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpectation_sites\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_constants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocal_terms\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_terms\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautodiff_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# the first step involves compiling the computation, which might take some time and print some (ignorable) warnings:\u001b[39;00m\n\u001b[1;32m     34\u001b[0m tnopt\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/local/src/MERA-Hubbard/.venv/lib/python3.12/site-packages/quimb/tensor/optimize.py:1225\u001b[0m, in \u001b[0;36mTNOptimizer.__init__\u001b[0;34m(self, tn, loss_fn, norm_fn, loss_constants, loss_kwargs, tags, shared_tags, constant_tags, loss_target, optimizer, progbar, bounds, autodiff_backend, executor, callback, **backend_opts)\u001b[0m\n\u001b[1;32m   1223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandler \u001b[38;5;241m=\u001b[39m MultiLossHandler(autodiff_backend, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbackend_opts)\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1225\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandler \u001b[38;5;241m=\u001b[39m \u001b[43m_BACKEND_HANDLERS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mautodiff_backend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbackend_opts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;66;03m# use identity if no nomalization required\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m norm_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/local/src/MERA-Hubbard/.venv/lib/python3.12/site-packages/quimb/tensor/optimize.py:609\u001b[0m, in \u001b[0;36mTorchHandler.__init__\u001b[0;34m(self, jit_fn, device)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, jit_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 609\u001b[0m     torch \u001b[38;5;241m=\u001b[39m \u001b[43mget_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit_fn \u001b[38;5;241m=\u001b[39m jit_fn\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/local/src/MERA-Hubbard/.venv/lib/python3.12/site-packages/quimb/tensor/optimize.py:602\u001b[0m, in \u001b[0;36mget_torch\u001b[0;34m()\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_torch\u001b[39m():\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "max_bond = 2\n",
    "n = num_sites * 2\n",
    "print(f'\\n*************** OPTIMIZATION MERA - MAX_BOND = {max_bond} ***************\\n')\n",
    "# Initialization MERA\n",
    "mera = qtn.MERA.rand(n, max_bond=max_bond) #, dangle=True) # the dangle option determines if the last isometry has an external edge not connected to anything else.. it affects the dimension of the tensor!!!\n",
    "mera.isometrize_()\n",
    "\n",
    "# build local terms to then compute the energy term by term (needed for the optimization of the MERA)\n",
    "local_terms = ham.build_local_terms()\n",
    "\n",
    "# To find a high quality contraction path for each term \n",
    "opt = ctg.ReusableHyperOptimizer(\n",
    "    progbar=True,\n",
    "    reconf_opts={},\n",
    "    max_repeats=16,\n",
    "    # directory=  # set this for persistent cache\n",
    ")\n",
    "\n",
    "expectation_sites(mera, local_terms, optimize=opt)\n",
    "expectation_sites(mera, local_terms, optimize=opt)\n",
    "\n",
    "# REAL OPTIMIZATION\n",
    "# set-up the MERA optimizer object:\n",
    "tnopt = qtn.TNOptimizer(\n",
    "    mera,\n",
    "    loss_fn=expectation_sites, \n",
    "    norm_fn=norm_fn,\n",
    "    loss_constants={'local_terms': local_terms},\n",
    "    loss_kwargs={'optimize': opt},\n",
    "    autodiff_backend='torch', device='cpu', jit_fn=True, \n",
    ")\n",
    "\n",
    "# the first step involves compiling the computation, which might take some time and print some (ignorable) warnings:\n",
    "tnopt.optimize(1)\n",
    "\n",
    "tnopt.optimizer = 'l-bfgs-b'  # the default\n",
    "mera_opt_hubbard = tnopt.optimize(999)\n",
    "\n",
    "tnopt.optimizer = 'adam'  # useful for final iterations\n",
    "mera_opt_hubbard = tnopt.optimize(1000)    \n",
    "\n",
    "en_mera = expectation_sites(mera_opt_hubbard, local_terms, optimize=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the tensors layer by layer\n",
    "new_tensors = []\n",
    "tensors_dict = {\"tensors\": [], \"qubits\": [], \"tags\": []}\n",
    "jump = 1\n",
    "ind = 0\n",
    "start_ind = 0\n",
    "for n_layer in np.arange(0,np.log2(mera_opt_hubbard.nsites)):\n",
    "    # print(f'************* _LAYER{int(n_layer)} *************')\n",
    "    mera_layer = mera_opt_hubbard.select([f'_LAYER{int(n_layer)}'], 'all')\n",
    "    node_shape = dict([('_UNI','o'), ('_ISO','v')])\n",
    "    # mera_layer.draw(color=['_UNI', '_ISO'], show_inds='all', show_tags=False, node_shape=node_shape)\n",
    "    \n",
    "    # select only the unitaries\n",
    "    mera_l_uni = mera_layer.select(['_UNI'], 'all')\n",
    "\n",
    "    for tn in mera_l_uni.tensors:\n",
    "        # tn = tn.data\n",
    "        # if '_UNI' in tn.tags: \n",
    "        ind_k1 = tn.inds[0]\n",
    "        ind_k2 = tn.inds[1]\n",
    "        ind_ex1 = tn.inds[2]\n",
    "        ind_ex2 = tn.inds[3]\n",
    "        new_tn = tn.fuse({f'ind_int':(f'{ind_k1}', f'{ind_k2}')})\n",
    "        new_tn = new_tn.fuse({f'ind_ext':(f'{ind_ex1}', f'{ind_ex2}')})\n",
    "        # print(new_tn)\n",
    "        new_tensors.append(new_tn.data.tolist())\n",
    "        if ind+jump >= n:\n",
    "            tensors_dict[\"tensors\"].append(new_tn.data.tolist())\n",
    "            tensors_dict[\"qubits\"].append([ind, ind+jump-n])\n",
    "            tensors_dict[\"tags\"].append(\"UNI\")\n",
    "        else:\n",
    "            tensors_dict[\"tensors\"].append(new_tn.data.tolist())\n",
    "            tensors_dict[\"qubits\"].append([ind, ind+jump])\n",
    "            tensors_dict[\"tags\"].append(\"UNI\")\n",
    "        ind += (jump*2)\n",
    "        if ind >= n:\n",
    "            ind = ind - n\n",
    "        # print('UNI appended')\n",
    "\n",
    "    # select only the isometries\n",
    "    mera_l_iso = mera_layer.select(['_ISO'], 'all')\n",
    "    \n",
    "    ind = start_ind + jump\n",
    "    for tn in mera_l_iso.tensors:\n",
    "        # tn = tn.data\n",
    "        # if '_ISO' in tn.tags: \n",
    "        if len(tn.inds) >= 3:\n",
    "            ind_in1 = tn.inds[0]\n",
    "            ind_in2 = tn.inds[1]\n",
    "            ind_ex = tn.inds[2]\n",
    "            new_tn = tn.fuse({f'ind_int':(f'{ind_in1}', f'{ind_in2}')})\n",
    "            # print(new_tn.data)\n",
    "            v1 = new_tn.data[:,0]\n",
    "            v2 = new_tn.data[:,1]\n",
    "            rnd1 = np.random.randn(4)  # take a random vector\n",
    "            rnd2 = np.random.randn(4)  # take a random vector\n",
    "\n",
    "            u0 = v1 # cannot change\n",
    "            u1 = v2 # cannot change\n",
    "            u2 = rnd1 - np.dot(u0,rnd1)*u0 - np.dot(u1,rnd1)*u1\n",
    "            u3 = rnd2 - np.dot(u0,rnd2)*u0 - np.dot(u1,rnd2)*u1 - np.dot(u2,rnd2)*u2/np.dot(u2,u2)\n",
    "            u2 /= np.linalg.norm(u2)  # normalize it\n",
    "            u3 /= np.linalg.norm(u3)  # normalize it\n",
    "\n",
    "            new_tn_completed = np.zeros((4,4))\n",
    "            new_tn_completed[:,0:2] = new_tn.data\n",
    "            new_tn_completed[:,2] = u2\n",
    "            new_tn_completed[:,3] = u3\n",
    "            # print(new_tn_completed)\n",
    "\n",
    "        else: # last tensor, it has one index less\n",
    "            ind_in1 = tn.inds[0]\n",
    "            ind_in2 = tn.inds[1]\n",
    "            new_tn = tn.fuse({f'ind_int':(f'{ind_in1}', f'{ind_in2}')})\n",
    "            # print(new_tn.data)\n",
    "            v1 = new_tn.data\n",
    "            rnd1 = np.random.randn(4)  # take a random vector\n",
    "            rnd2 = np.random.randn(4)  # take a random vector\n",
    "            rnd3 = np.random.randn(4)  # take a random vector\n",
    "\n",
    "            u0 = v1 # cannot change\n",
    "            u1 = rnd1 - np.dot(u0,rnd1)*u0\n",
    "            u2 = rnd2 - np.dot(u0,rnd2)*u0/np.dot(u0,u0) - np.dot(u1,rnd2)*u1/np.dot(u1,u1)\n",
    "            u3 = rnd3 - np.dot(u0,rnd3)*u0/np.dot(u0,u0) - np.dot(u1,rnd3)*u1/np.dot(u1,u1) - np.dot(u2,rnd3)*u2/np.dot(u2,u2)\n",
    "            u1 /= np.linalg.norm(u1)  # normalize it\n",
    "            u2 /= np.linalg.norm(u2)  # normalize it\n",
    "            u3 /= np.linalg.norm(u3)  # normalize it\n",
    "\n",
    "            new_tn_completed = np.zeros((4,4))\n",
    "            new_tn_completed[:,0] = new_tn.data\n",
    "            new_tn_completed[:,1] = u1\n",
    "            new_tn_completed[:,2] = u2\n",
    "            new_tn_completed[:,3] = u3\n",
    "                        \n",
    "            # new_tn_completed = tn.data\n",
    "        new_tensors.append(new_tn_completed.tolist())\n",
    "        if ind+jump >= n:\n",
    "            tensors_dict[\"tensors\"].append(new_tn_completed.data.tolist())\n",
    "            tensors_dict[\"qubits\"].append([ind, ind+jump-n])\n",
    "            tensors_dict[\"tags\"].append(\"ISO\")\n",
    "        else:\n",
    "            tensors_dict[\"tensors\"].append(new_tn_completed.data.tolist())\n",
    "            tensors_dict[\"qubits\"].append([ind, ind+jump])\n",
    "            tensors_dict[\"tags\"].append(\"ISO\")\n",
    "        ind += (jump*2)\n",
    "        if ind >= n:\n",
    "            ind = ind - n\n",
    "        # print('ISO appended')\n",
    "    \n",
    "    start_ind += int(2**n_layer)\n",
    "    jump += int(2**n_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_tensors = []\n",
    "# previous_inds = []\n",
    "# for i, tn in enumerate(mera_opt_hubbard.tensors):\n",
    "#     tags = []\n",
    "#     for t in tn.tags:\n",
    "#         tags.append(t)\n",
    "#     # print(tags)\n",
    "    \n",
    "#     if '_UNI' in tags: # and '_LAYER0' in tags:\n",
    "#         ind_k1 = tn.inds[0]\n",
    "#         ind_k2 = tn.inds[1]\n",
    "#         ind_ex1 = tn.inds[2]\n",
    "#         ind_ex2 = tn.inds[3]\n",
    "#         new_tn = tn.fuse({f'ind_k_{i}':(f'{ind_k1}', f'{ind_k2}')})\n",
    "#         new_tn = new_tn.fuse({f'ind_ex_{i}':(f'{ind_ex1}', f'{ind_ex2}')})\n",
    "#         previous_inds.append([ind_ex1, ind_ex2])\n",
    "#         # print(new_tn)\n",
    "#         new_tensors.append(new_tn.data.tolist())\n",
    "\n",
    "#     if '_ISO' in tags: # and '_LAYER0' in tags:\n",
    "\n",
    "#         if len(tn.inds) >= 3:\n",
    "#             ind_in1 = tn.inds[0]\n",
    "#             ind_in2 = tn.inds[1]\n",
    "#             ind_ex = tn.inds[2]\n",
    "#             new_tn = tn.fuse({f'ind_in_{i}':(f'{ind_in1}', f'{ind_in2}')})\n",
    "#             previous_inds.append([])\n",
    "#             # print(new_tn.data)\n",
    "#             v1 = new_tn.data[:,0]\n",
    "#             v2 = new_tn.data[:,1]\n",
    "#             rnd1 = np.random.randn(4)  # take a random vector\n",
    "#             rnd2 = np.random.randn(4)  # take a random vector\n",
    "\n",
    "#             u0 = v1 # cannot change\n",
    "#             u1 = v2 # cannot change\n",
    "#             u2 = rnd1 - np.dot(u0,rnd1)*u0 - np.dot(u1,rnd1)*u1\n",
    "#             u3 = rnd2 - np.dot(u0,rnd2)*u0 - np.dot(u1,rnd2)*u1 - np.dot(u2,rnd2)*u2/np.dot(u2,u2)\n",
    "#             u2 /= np.linalg.norm(u2)  # normalize it\n",
    "#             u3 /= np.linalg.norm(u3)  # normalize it\n",
    "\n",
    "#             new_tn_completed = np.zeros((4,4))\n",
    "#             new_tn_completed[:,0:2] = new_tn.data\n",
    "#             new_tn_completed[:,2] = u2\n",
    "#             new_tn_completed[:,3] = u3\n",
    "#             # print(new_tn_completed)\n",
    "\n",
    "#         else: # last tensor, it has one index less\n",
    "#             ind_in1 = tn.inds[0]\n",
    "#             ind_in2 = tn.inds[1]\n",
    "#             new_tn = tn.fuse({f'ind_in_{i}':(f'{ind_in1}', f'{ind_in2}')})\n",
    "#             previous_inds.append([])\n",
    "#             # print(new_tn.data)\n",
    "#             v1 = new_tn.data\n",
    "#             rnd1 = np.random.randn(4)  # take a random vector\n",
    "#             rnd2 = np.random.randn(4)  # take a random vector\n",
    "#             rnd3 = np.random.randn(4)  # take a random vector\n",
    "\n",
    "#             u0 = v1 # cannot change\n",
    "#             u1 = rnd1 - np.dot(u0,rnd1)*u0\n",
    "#             u2 = rnd2 - np.dot(u0,rnd2)*u0/np.dot(u0,u0) - np.dot(u1,rnd2)*u1/np.dot(u1,u1)\n",
    "#             u3 = rnd3 - np.dot(u0,rnd3)*u0/np.dot(u0,u0) - np.dot(u1,rnd3)*u1/np.dot(u1,u1) - np.dot(u2,rnd3)*u2/np.dot(u2,u2)\n",
    "#             u1 /= np.linalg.norm(u1)  # normalize it\n",
    "#             u2 /= np.linalg.norm(u2)  # normalize it\n",
    "#             u3 /= np.linalg.norm(u3)  # normalize it\n",
    "\n",
    "#             new_tn_completed = np.zeros((4,4))\n",
    "#             new_tn_completed[:,0] = new_tn.data\n",
    "#             new_tn_completed[:,1] = u1\n",
    "#             new_tn_completed[:,2] = u2\n",
    "#             new_tn_completed[:,3] = u3\n",
    "                        \n",
    "#             # new_tn_completed = tn.data\n",
    "#         new_tensors.append(new_tn_completed.tolist())\n",
    "\n",
    "#         # checks \n",
    "#         # print(np.round(np.dot(new_tn_completed.transpose(),new_tn_completed),3))\n",
    "#         # print(np.round(np.dot(new_tn_completed,new_tn_completed.transpose())))\n",
    "\n",
    "#         # print('det = ', np.linalg.det(new_tn_completed))\n",
    "#         # print('norm row = ', np.linalg.norm(new_tn_completed[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dictionary as .json file, easier to upload in qiskit..\n",
    "import json\n",
    "\n",
    "# mera_tensors = dict()\n",
    "# for ind, tn in enumerate(new_tensors):\n",
    "#     # print(tn.left_inds) # indices of the edges that 'enter' the node\n",
    "#     # print(tn.get_params())\n",
    "#     mera_tensors[ind] = tn\n",
    "\n",
    "# Writing to sample.json\n",
    "with open(f\"not-optMERA_1x{num_sites}_U={U}_t2={t2}.json\", \"w\") as outfile:\n",
    "    # Serializing json\n",
    "    json_object = json.dumps(tensors_dict)\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Opening JSON file\n",
    "# with open(f\"not-optMERA_1x{num_sites}_U={U}_t2={t2}.json\", 'r') as openfile:\n",
    "#     # Reading from json file\n",
    "#     mera_json = json.load(openfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
